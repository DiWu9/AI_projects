{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Analysis\n",
    "\n",
    "## Introduction\n",
    "The purpose of this assignment is to practice implementing Q-Learning in increasing challenging environments and reinforce the understanding of reinforcement learning and Agent-Environment interactions.\n",
    "\n",
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import QLearningAgent as qa\n",
    "import QLearningAgent2 as qa2\n",
    "import QLearningAgentTaxi as qat\n",
    "\n",
    "import numpy as np\n",
    "import coffeegame\n",
    "import math\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "### QLearningAgent (fixed exploration rate)\n",
    "#### Learning Output & Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mCH\n",
      "DHC\n",
      "HHG\n",
      "  (Right)\n",
      "S\u001b[41mC\u001b[0mH\n",
      "DHC\n",
      "HHG\n",
      "  (Down)\n",
      "SCH\n",
      "D\u001b[41mH\u001b[0mC\n",
      "HHG\n",
      "  (Right)\n",
      "SCH\n",
      "DH\u001b[41mC\u001b[0m\n",
      "HHG\n",
      "  (Down)\n",
      "SCH\n",
      "DHC\n",
      "HH\u001b[41mG\u001b[0m\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "H\u001b[41mF\u001b[0mFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agent_coffee = qa.QLearningAgent(coffeegame.CoffeeEnv())\n",
    "agent_coffee.init()\n",
    "\n",
    "env = gym.make(\"FrozenLake-v0\", is_slippery = False)\n",
    "agent_frozen = qa.QLearningAgent(env)\n",
    "agent_frozen.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Best Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 0.4 0.4\n",
      "0.05 0.4 0.45\n",
      "0.05 0.4 0.5\n",
      "0.05 0.4 0.55\n",
      "0.05 0.4 0.6\n",
      "0.05 0.5 0.4\n",
      "0.05 0.5 0.45\n",
      "0.05 0.5 0.5\n",
      "0.05 0.5 0.55\n",
      "0.05 0.5 0.6\n",
      "0.05 0.6000000000000001 0.4\n",
      "0.05 0.6000000000000001 0.45\n",
      "0.05 0.6000000000000001 0.5\n",
      "0.05 0.6000000000000001 0.55\n",
      "0.05 0.6000000000000001 0.6\n",
      "0.05 0.7000000000000001 0.4\n",
      "0.05 0.7000000000000001 0.45\n",
      "0.05 0.7000000000000001 0.5\n",
      "0.05 0.7000000000000001 0.55\n",
      "0.05 0.7000000000000001 0.6\n",
      "0.05 0.8 0.4\n",
      "0.05 0.8 0.45\n",
      "0.05 0.8 0.5\n",
      "0.05 0.8 0.55\n",
      "0.05 0.8 0.6\n",
      "0.1 0.4 0.4\n",
      "0.1 0.4 0.45\n",
      "0.1 0.4 0.5\n",
      "0.1 0.4 0.55\n",
      "0.1 0.4 0.6\n",
      "0.1 0.5 0.4\n",
      "0.1 0.5 0.45\n",
      "0.1 0.5 0.5\n",
      "0.1 0.5 0.55\n",
      "0.1 0.5 0.6\n",
      "0.1 0.6000000000000001 0.4\n",
      "0.1 0.6000000000000001 0.45\n",
      "0.1 0.6000000000000001 0.5\n",
      "0.1 0.6000000000000001 0.55\n",
      "0.1 0.6000000000000001 0.6\n",
      "0.1 0.7000000000000001 0.4\n",
      "0.1 0.7000000000000001 0.45\n",
      "0.1 0.7000000000000001 0.5\n",
      "0.1 0.7000000000000001 0.55\n",
      "0.1 0.7000000000000001 0.6\n",
      "0.1 0.8 0.4\n",
      "0.1 0.8 0.45\n",
      "0.1 0.8 0.5\n",
      "0.1 0.8 0.55\n",
      "0.1 0.8 0.6\n",
      "0.15000000000000002 0.4 0.4\n",
      "0.15000000000000002 0.4 0.45\n",
      "0.15000000000000002 0.4 0.5\n",
      "0.15000000000000002 0.4 0.55\n",
      "0.15000000000000002 0.4 0.6\n",
      "0.15000000000000002 0.5 0.4\n",
      "0.15000000000000002 0.5 0.45\n",
      "0.15000000000000002 0.5 0.5\n",
      "0.15000000000000002 0.5 0.55\n",
      "0.15000000000000002 0.5 0.6\n",
      "0.15000000000000002 0.6000000000000001 0.4\n",
      "0.15000000000000002 0.6000000000000001 0.45\n",
      "0.15000000000000002 0.6000000000000001 0.5\n",
      "0.15000000000000002 0.6000000000000001 0.55\n",
      "0.15000000000000002 0.6000000000000001 0.6\n",
      "0.15000000000000002 0.7000000000000001 0.4\n",
      "0.15000000000000002 0.7000000000000001 0.45\n",
      "0.15000000000000002 0.7000000000000001 0.5\n",
      "0.15000000000000002 0.7000000000000001 0.55\n",
      "0.15000000000000002 0.7000000000000001 0.6\n",
      "0.15000000000000002 0.8 0.4\n",
      "0.15000000000000002 0.8 0.45\n",
      "0.15000000000000002 0.8 0.5\n",
      "0.15000000000000002 0.8 0.55\n",
      "0.15000000000000002 0.8 0.6\n",
      "0.2 0.4 0.4\n",
      "0.2 0.4 0.45\n",
      "0.2 0.4 0.5\n",
      "0.2 0.4 0.55\n",
      "0.2 0.4 0.6\n",
      "0.2 0.5 0.4\n",
      "0.2 0.5 0.45\n",
      "0.2 0.5 0.5\n",
      "0.2 0.5 0.55\n",
      "0.2 0.5 0.6\n",
      "0.2 0.6000000000000001 0.4\n",
      "0.2 0.6000000000000001 0.45\n",
      "0.2 0.6000000000000001 0.5\n",
      "0.2 0.6000000000000001 0.55\n",
      "0.2 0.6000000000000001 0.6\n",
      "0.2 0.7000000000000001 0.4\n",
      "0.2 0.7000000000000001 0.45\n",
      "0.2 0.7000000000000001 0.5\n",
      "0.2 0.7000000000000001 0.55\n",
      "0.2 0.7000000000000001 0.6\n",
      "0.2 0.8 0.4\n",
      "0.2 0.8 0.45\n",
      "0.2 0.8 0.5\n",
      "0.2 0.8 0.55\n",
      "0.2 0.8 0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.1, 0.6000000000000001, 0.4], 0.8729243278503418]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import find_best_parameter as fbp\n",
    "\n",
    "fbp.findBestParameter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **[findBestParameter()](http://localhost:8888/edit/project-6-q/find_best_parameter.py)** function trys different combinations of alpha, gamma, and epsilon of **[QLearningAgent](http://localhost:8888/edit/project-6-q/QLearningAgent.py)** and compares their efficiency. The run-time is calculated for each learning and the best result is the combination with least run-time, which is **alpha = 0.1, gamma = 0.6, epsilon = 0.4** with approximate run-time **0.87**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningAgent2 (with decay rate)\n",
    "#### Learning Output & Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mCH\n",
      "DHC\n",
      "HHG\n",
      "  (Right)\n",
      "S\u001b[41mC\u001b[0mH\n",
      "DHC\n",
      "HHG\n",
      "  (Down)\n",
      "SCH\n",
      "D\u001b[41mH\u001b[0mC\n",
      "HHG\n",
      "  (Right)\n",
      "SCH\n",
      "DH\u001b[41mC\u001b[0m\n",
      "HHG\n",
      "  (Down)\n",
      "SCH\n",
      "DHC\n",
      "HH\u001b[41mG\u001b[0m\n",
      "[[  0.752      -10.           2.92         0.752     ]\n",
      " [  0.752        3.2          3.2          2.92      ]\n",
      " [  2.92         7.           3.2          3.2       ]\n",
      " [  0.           0.           0.           0.        ]\n",
      " [-10.           5.           7.           2.92      ]\n",
      " [  3.2         10.           7.           3.2       ]\n",
      " [  1.38217909   0.96565957   4.99075813  -9.35389181]\n",
      " [  1.97684735   4.99898701  10.           3.1999286 ]\n",
      " [  0.           0.           0.           0.        ]]\n",
      "total reward:  11\n",
      "steps made:  [2, 1, 2, 1]\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "[[0.046656   0.07776    0.07776    0.046656  ]\n",
      " [0.04665583 0.         0.1296     0.07775945]\n",
      " [0.07775524 0.216      0.07768182 0.12959985]\n",
      " [0.12957167 0.         0.06645285 0.05536058]\n",
      " [0.07776    0.1296     0.         0.046656  ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.36       0.         0.12959855]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.1296     0.         0.216      0.07776   ]\n",
      " [0.1296     0.36       0.36       0.        ]\n",
      " [0.216      0.6        0.         0.216     ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.3596318  0.6        0.21562244]\n",
      " [0.36       0.6        1.         0.36      ]\n",
      " [0.         0.         0.         0.        ]]\n",
      "total reward:  1.0\n",
      "steps made:  [1, 1, 2, 2, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "agent_coffee_2 = qa2.QLearningAgent(coffeegame.CoffeeEnv())\n",
    "agent_coffee_2.init('red', 'coffee game')\n",
    "\n",
    "env_2 = gym.make(\"FrozenLake-v0\", is_slippery = False)\n",
    "agent_frozen_2 = qa2.QLearningAgent(env_2)\n",
    "agent_frozen_2.init('blue', 'frozen lake')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Result\n",
    "![average reward with dacay rate](decay_average_reward.png)\n",
    "\n",
    "#### Analysis\n",
    "- According to the graph result, the average reward (of each 1000 episode) in both the coffee game (red) and the FrozenLake (blue) is increasing.\n",
    "- The coffee game has more obvious result than FrozenLake because the max reward of coffeegame is 11 while the max reward of FrozenLake is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearningAgentTaxi\n",
    "#### Learning Output & Q-Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :G|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[42mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "|\u001b[42m_\u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "[[  0.           0.           0.           0.           0.\n",
      "    0.        ]\n",
      " [ -2.41837328  -2.36396237  -2.41837381  -2.36395128  -2.27325184\n",
      "  -11.36395578]\n",
      " [ -1.87014486  -1.45024004  -1.8701441   -1.45024001  -0.7504\n",
      "  -10.45024007]\n",
      " ...\n",
      " [ -0.7504222    0.416       -0.7504874   -1.45026547  -9.75042314\n",
      "   -9.75047823]\n",
      " [ -2.27395684  -2.12208643  -2.27508856  -2.12280754 -11.27406117\n",
      "  -11.27356997]\n",
      " [  5.5998828    2.35980552   5.59981441  11.          -3.40022784\n",
      "   -3.40020221]]\n",
      "total reward:  11\n",
      "steps made:  [3, 3, 1, 1, 4, 0, 0, 0, 0, 5]\n"
     ]
    }
   ],
   "source": [
    "taxi_game = gym.make(\"Taxi-v3\")\n",
    "agent_taxi = qat.QLearningAgent(taxi_game)\n",
    "agent_taxi.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Result\n",
    "![average_reward_taxi](decay_average_reward_taxi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "- The taxi-game is nondeterministic, which indicates that Q-Learning may be able to learn the game with uncertainties.\n",
    "- The average reward during learning is increasing when the exploration rate is decreasing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
